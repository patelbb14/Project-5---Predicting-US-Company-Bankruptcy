





# Imports
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.ticker as ticker
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import accuracy_score, confusion_matrix


from sklearn.preprocessing import StandardScaler

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, LSTM, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import BatchNormalization
from tensorflow.keras.callbacks import ReduceLROnPlateau

from keras.preprocessing.sequence import pad_sequences
from imblearn.over_sampling import SMOTE

from keras.regularizers import l2


# Read data
from google.colab import drive
drive.mount('/content/drive/')
file_path = '/content/drive/MyDrive/[1] DSI Drive/[2] Projects/Project 5/data/american_bankruptcy.csv'
df = pd.read_csv(file_path)


df['status_label'] = np.where(df['status_label'] == 'alive', 1, 0)


df['status_label'].value_counts()


df.head()


df.columns


import pandas as pd
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.callbacks import EarlyStopping
from sklearn.model_selection import train_test_split
import numpy as np
from keras.preprocessing.sequence import pad_sequences
from imblearn.over_sampling import SMOTE

# Assuming 'df' is your DataFrame

# Sort the DataFrame by company_name and year
df_sorted = df.sort_values(by=['company_name', 'year'])

# Group the data by company_name and create sequences
sequences = []
targets = []
for company, group in df_sorted.groupby('company_name'):
    features = group.drop(columns=['company_name', 'status_label', 'year']).values
    target = group['status_label'].values[-1]  # Get the last status_label for each company
    sequences.append(features)
    targets.append(target)

# Pad sequences to have the same length
max_sequence_length = max(len(sequence) for sequence in sequences)
sequences_padded = pad_sequences(sequences, maxlen=max_sequence_length, dtype='float64', padding='post')

# Convert targets to a numpy array
targets = np.array(targets)

# Flatten the sequences for SMOTE
sequences_flattened = sequences_padded.reshape(sequences_padded.shape[0], -1)

# Split the data into training and test sets
X_train_flat, X_test_flat, y_train, y_test = train_test_split(sequences_flattened, targets, test_size=0.2, random_state=42)

# Use SMOTE to oversample the minority class in the training set
smote = SMOTE(random_state=42)
X_train_res_flat, y_train_res = smote.fit_resample(X_train_flat, y_train)

# Reshape the data back into sequences
X_train_res = X_train_res_flat.reshape(-1, max_sequence_length, sequences_padded.shape[2])
X_test = X_test_flat.reshape(-1, max_sequence_length, sequences_padded.shape[2])

# Scale the data
scaler = StandardScaler()
X_train_res_scaled = scaler.fit_transform(X_train_res.reshape(-1, X_train_res.shape[-1])).reshape(X_train_res.shape)
X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)

# Build the RNN model
model = Sequential([
    LSTM(64, activation='relu', input_shape=(X_train_res_scaled.shape[1], X_train_res_scaled.shape[2])),
    Dropout(0.2),
    Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
history = model.fit(X_train_res_scaled, y_train_res, validation_data=(X_test_scaled, y_test),
                    epochs=50, batch_size=32, callbacks=[early_stopping])

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test)
print(f"Test accuracy: {test_accuracy:.4f}")



from keras.regularizers import l2

# Build the improved RNN model
model_improved = Sequential([
    LSTM(128, activation='relu', input_shape=(X_train_res_scaled.shape[1], X_train_res_scaled.shape[2]), return_sequences=True, kernel_regularizer=l2(0.001)),
    Dropout(0.3),
    LSTM(64, activation='relu', kernel_regularizer=l2(0.001)),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

# Compile the improved model
model_improved.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])

# Train the improved model
history_improved = model_improved.fit(X_train_res_scaled, y_train_res, validation_data=(X_test_scaled, y_test),
                                      epochs=50, batch_size=32, callbacks=[early_stopping])

# Evaluate the improved model
test_loss_improved, test_accuracy_improved = model_improved.evaluate(X_test_scaled, y_test)
print(f"Improved test accuracy: {test_accuracy_improved:.4f}")



from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout, BatchNormalization
from keras.callbacks import EarlyStopping
from keras.optimizers import Adam

# Build the improved RNN model
model_improved_2 = Sequential([
    LSTM(64, activation='relu', input_shape=(X_train_res_scaled.shape[1], X_train_res_scaled.shape[2]), return_sequences=True),
    Dropout(0.3),
    BatchNormalization(),
    LSTM(32, activation='relu'),
    Dropout(0.3),
    BatchNormalization(),
    Dense(1, activation='sigmoid')
])

# Compile the model
model_improved_2.compile(optimizer=Adam(learning_rate=0.0005), loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
history_improved_2 = model_improved_2.fit(X_train_res_scaled, y_train_res, validation_data=(X_test_scaled, y_test),
                                          epochs=50, batch_size=32, callbacks=[early_stopping])

# Evaluate the model
test_loss, test_accuracy = model_improved_2.evaluate(X_test_scaled, y_test)
print(f"Improved test accuracy: {test_accuracy:.4f}")

